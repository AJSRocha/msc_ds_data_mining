---
title: "R Notebook"
output: github_document
---

```{r}
library(dplyr)
library(ggplot2)
library(wordcloud)
library(text2vec)
library(tm)
##
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
##
library(caret)
library(MLeval)
```

```{r data_import}
Sys.getlocale()

readr::guess_encoding('FN-Dataset-18k//FN-Dataset-18k.csv')
df = read.csv('FN-Dataset-18k//FN-Dataset-18k.csv',encoding = 'UTF-8') %>% 
  mutate(y = factor(questionable_domain))

```

# EDA

```{r}
df %>% 
  filter(title != description) %>% 
  View

sum(df$title == df$description)
```

# Silly plots

```{r}



```

# Tokenizing

```{r}
corpus = quanteda::corpus(df$description)
tweets_tokens = quanteda::tokens(tolower(corpus),
                        remove_punct = TRUE,
                        split_hyphens = TRUE)

tweets_clean = tokens_wordstem(tweets_tokens) %>%
  dfm(., tolower = TRUE) %>% 
  dfm_remove(., stopwords("english"))

tweets_clean = tweets_clean[,order(featnames(tweets_clean))]

dfm = dfm(tweets_clean)
```

```{r}
topfeatures(dfm)

textstat_frequency(tweets_clean, n = 25, groups = df$questionable_domain) %>% 
  ggplot(aes(x=reorder(feature, frequency),
           y=frequency, fill=group)) +
  geom_bar(stat = "identity",
           position = position_dodge2(width = 0.9, preserve = "single")) +
  scale_fill_manual(values = c("darkgreen", "red")) +
  coord_flip() +
  theme(legend.position = "none") +
  labs(x=NULL, y = "Frequency") +
  theme_minimal() + 
  theme(legend.position = 'left')
```

# Silly wordclouds

```{r}
fake_news = df %>%
  filter(questionable_domain)
fake_news = quanteda::corpus(fake_news$description) %>% 
  quanteda::tokens(remove_punct = TRUE, split_hyphens = TRUE) %>% 
  dfm

textplot_wordcloud(fake_news)
```
```{r}
great_news = df %>%
  filter(!questionable_domain)
great_news = quanteda::corpus(great_news$description) %>% 
  quanteda::tokens(remove_punct = TRUE, split_hyphens = TRUE) %>% 
  dfm

textplot_wordcloud(great_news, color = 'red')
```

# Transform tweets into features

```{r}
featurizer = function(token, data){
  res = dfm_select(dfm, pattern = token)
  res = convert(res, to = 'data.frame')
  res = ifelse(res[,token] > 0, 1, 0) 
  res = cbind(data, res)
  names(res)[length(names(res))] = token
  return(res)
  }
```

## Some tokens:

```{r}
df_ex = featurizer('@gop', df) %>% 
  featurizer('biden',.) %>% 
    featurizer('trump',.) %>%
  featurizer('elect',.) %>% 
  featurizer('@gatewaypundit',.)
  


```

## Test - Train split

### Option 1: Blind partition:

```{r}
index = caret::createDataPartition(df$questionable_domain, p = 0.2, list = F)
train = df_ex[index,]
test = df_ex[-index,]
```

### Option2: Restructure dataset:

```{r}
set.seed(123)
# true tweets:
index_t = which(df$questionable_domain)

# false tweets:
index_f = which(!df$questionable_domain) %>% 
  sample(6000)

index_r = c(index_t,index_t,index_f)

df_ex_r = df_ex[index_r,]
index = caret::createDataPartition(df_ex_r$questionable_domain, p = 0.2, list = F)

train = df_ex_r[index,]
test = df_ex_r[-index,]
```

## Models fit

```{r}
# common parameters:
y = test$questionable_domain

modelo = as.formula(y ~ `@gop` + biden + trump + elect + `@gatewaypundit`)

ctrl = trainControl(method = "cv",
                      number = 10,
                      savePredictions = T)

# model-specific grid:
grelha = expand.grid(mtry = c(1,2,5,10,20,30,40,50))


```

## Naive Bayes

```{r}
model_nb = train(modelo,
                 data = train,
                 method = "nb",
                 trControl = ctrl)

```

## Random Forest

```{r}
# corremos a simulação
model_rf = caret::train(modelo,
                     method="rf",
                     data=train,
                     trControl=ctrl,
                     #tuneGrid=grelha,
                     #preProc=c("BoxCox"),
                     ntree=500,
                     metric="Accuracy")



print(forest)
forest$finalModel
forest$results

```


## SVM

```{r}

ctrl = trainControl(method="cv", summaryFunction=twoClassSummary, classProbs=T,
                     savePredictions = T)

model_svm =
train(modelo,
      data = train,
      method = "svmLinear",
      control = ctrl)

model_svm$finalModel
model_svm$pred
print(model_svm)

roc = evalm(model_svm)

confusionMatrix(model_svm)
confusionMatrix(predict(model_svm, newdata = test),
                factor(test$questionable_domain),
                mode = 'everything')

```

## Evaluation

```{r}
# https://intobioinformatics.wordpress.com/2019/11/26/how-to-easily-make-a-roc-curve-in-r/

notas = evalm(list(model_rf, model_svm), gnames=c('rf','svm'))

```


# PREDICT NEW TWEETS

```{r}

# pre-processing function
conversor = function(new, ml_model){
  temp = quanteda::corpus(new$description)
  tokens = quanteda::tokens(temp,
                        remove_punct = TRUE,
                        split_hyphens = TRUE)
  # builds output
  predictable = new %>% 
    mutate(`@gop` = ifelse('@gop' %in% tokens, 1,0),
           biden = ifelse('biden' %in% tokens, 1,0),
           trump = ifelse('trump' %in% tokens, 1,0))
  
  if(ml_model == 'rf'){
      output = predict(model_rf, newdata = predictable)
  }
  else if(ml_model == 'svm'){
    output = predict(model_svm, newdata = predictable)
  }
  
  return(output)
}

# create dummy tweets
dummy = df[3,]

conversor(dummy, 'rf')

```






