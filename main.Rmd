---
title: "R Notebook"
output: github_document
---

```{r}
library(dplyr)
library(ggplot2)
library(wordcloud)
library(text2vec)
library(tm)
##
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
##
library(caret)
library(MLeval)
```

```{r data_import}
Sys.getlocale()

readr::guess_encoding('FN-Dataset-18k//FN-Dataset-18k.csv')
df = read.csv('FN-Dataset-18k//FN-Dataset-18k.csv',encoding = 'UTF-8') %>% 
  mutate(y = factor(questionable_domain))

```

# Tokenizing

```{r}
corpus = quanteda::corpus(df$description)
tweets_tokens = quanteda::tokens(tolower(corpus),
                        remove_punct = TRUE,
                        split_hyphens = TRUE)

tweets_clean = tokens_wordstem(tweets_tokens) %>%
  dfm(., tolower = TRUE) %>% 
  dfm_remove(., stopwords("english"))

tweets_clean = tweets_clean[,order(featnames(tweets_clean))]

dfm = dfm(tweets_clean)

# subsets for EDA
hashtags = dfm_select(tweets_clean, pattern = '#*')
mention = dfm_select(tweets_clean, pattern = '@*')
normals = dfm_remove(tweets_clean, pattern = c('#*','@*'))
```

# EDA

```{r}
pesquisa = dfm_select(tweets_clean, pattern = '#gop*')
pesquisa@Dimnames$features
```


## Initial Features

```{r}
df %>% 
  filter(title != description) %>% 
  View

sum(df$title == df$description)
```

## Tokens

### Overall:

```{r}

textstat_frequency(tweets_clean, n = 25, groups = df$questionable_domain) %>% 
  ggplot(aes(x=reorder(feature, frequency),
           y=frequency, fill=group)) +
  geom_bar(stat = "identity",
           position = position_dodge2(width = 0.9, preserve = "single")) +
  scale_fill_manual(values = c("darkgreen", "red")) +
  coord_flip() +
  labs(x=NULL, y = "Frequency") +
  theme_minimal() + 
  theme(legend.position = 'none')
```

### Hashtags:

```{r}
textstat_frequency(hashtags, n = 25, groups = df$questionable_domain) %>% 
  ggplot(aes(x=reorder(feature, frequency),
           y=frequency, fill=group)) +
  geom_bar(stat = "identity",
           position = position_dodge2(width = 0.9, preserve = "single")) +
  scale_fill_manual(values = c("darkgreen", "red")) +
  coord_flip() +
  labs(x=NULL, y = "Frequency") +
  theme_minimal() + 
  theme(legend.position = 'none')
```

### Mentions:

```{r}
textstat_frequency(mention, n = 25, groups = df$questionable_domain) %>% 
  ggplot(aes(x=reorder(feature, frequency),
           y=frequency, fill=group)) +
  geom_bar(stat = "identity",
           position = position_dodge2(width = 0.9, preserve = "single")) +
  scale_fill_manual(values = c("darkgreen", "red")) +
  coord_flip() +
  labs(x=NULL, y = "Frequency") +
  theme_minimal() + 
  theme(legend.position = 'none')
```

### Normal words:

* vermelho = TRUE, verde = FALSE;

```{r}
textstat_frequency(normals, n = 25, groups = df$questionable_domain) %>% 
  ggplot(aes(x=reorder(feature, frequency),
           y=frequency, fill=group)) +
  geom_bar(stat = "identity",
           position = position_dodge2(width = 0.9, preserve = "single")) +
  scale_fill_manual(values = c("darkgreen", "red")) +
  coord_flip() +
  labs(x=NULL, y = "Frequency") +
  theme_minimal() + 
  theme(legend.position = 'none')
```

### Groups derived

```{r groups}
hash_t = c('#coronavirus', '#republican', '#bidenharris2020', '#covid', '#trumpvirus',
           '#trumpisanationaldisgrac', '#gopcorruptionovercountri','#moscowmitch', '#democraci',
           '#bidenharri', '#debat', '#votebluetosaveamerica')

hash_f = c('#trump2020', '#joebiden', '#stopthest', '#foxnew', '#electionfraud',
           '#democrat', '#aag2020', '#nytim', '#cnn', '#pennsylvania',
           '#nevertrump', '#michigan', '#america', '#8217')

mention_t = c('@senategop', '@joebiden', '@senatemajldr', '@housegop',
              '@yahoo', '@realdonaldtrump<U+2069>', '@gopchairwoman', '@gop<U+2069>',
              '@cnn', '@goplead', '@speakerpelosi', '@senatedem', '@vp', '@potus',
              '@whitehous', '@foxnew', '@lindseygrahamsc', '@kamalaharri', '@dnc',
              '@washingtonpost')

mention_f = c('@gatewaypundit', '@marklevinshow', '@oann', '@realjameswood', '@judgejeanin', '@tomfitton',
              '@govmikehuckabe', '@walkawayaus', '@patpenn2', '@nfulmer0827', '@msspy007',
              '@marylene58', '@lady2018cat', '@jessjackie50', '@freelion7', '@foundinnv',
              '@enemyoftheleft', '@donnawr8', '@briteeyes8', '@bluehandris', '@anthonymentill4',
              '@easterndmondbk')

norm_t = c('american', 'new', 'republican', 'us', 'just', 'get', 'like', 'go',
           'covid', 'now', 'lie', 'voter', 'york')

norm_f = c('video', 'joe', 'break', 'fraud', 'ballot', 'georgia',
           'hunter', 'report', 'michigan', 'show', 'news', 'releas', 'support',
           'democrat')
```


# Silly wordclouds

```{r}
fake_news = df %>%
  filter(questionable_domain)
fake_news = quanteda::corpus(fake_news$description) %>% 
  quanteda::tokens(remove_punct = TRUE, split_hyphens = TRUE) %>% 
  dfm %>% 
  dfm_remove(., stopwords("english"))

textplot_wordcloud(fake_news)
```

```{r}
great_news = df %>%
  filter(!questionable_domain)
great_news = quanteda::corpus(great_news$description) %>% 
  quanteda::tokens(remove_punct = TRUE, split_hyphens = TRUE) %>% 
  dfm %>% 
  dfm_remove(., stopwords("english"))

textplot_wordcloud(great_news, color = 'red')
```

```{r}
textplot_wordcloud(hashtags, color = 'darkgreen')
```

```{r}
textplot_wordcloud(mention, color = 'brown')
```

```{r}
textplot_wordcloud(normals, color = 'purple')
```


# Transform tweets into features

```{r}
featurizer = function(token, data){
  res = dfm_select(dfm, pattern = token)
  res = convert(res, to = 'data.frame')
  res = ifelse(res[,token] > 0, 1, 0) 
  res = cbind(data, res)
  names(res)[length(names(res))] = token
  return(res)
}

featurizer_group = function(group, group_id, data){
  # restricts to the desired tokens
  res = dfm_select(dfm, pattern = group)
  res = convert(res, to = 'data.frame')
  # calculates a group score: occurences / total
  res = rowSums(res[-1]/(length(group)))
  # attaches and format result:
  res = cbind(data, res)
  names(res)[length(names(res))] = group_id
  return(res)
}

```

## Some tokens:

```{r}
df_ex = featurizer('@gop', df) %>% 
  featurizer('biden',.) %>% 
    featurizer('trump',.) %>%
  featurizer('elect',.) %>% 
  featurizer('@gatewaypundit',.) %>% 
  featurizer_group(hash_f, 'hash_f',.) %>% 
  featurizer_group(hash_t, 'hash_t',.) %>% 
  featurizer_group(mention_t, 'mention_t',.) %>% 
  featurizer_group(mention_f, 'mention_f',.) %>% 
  featurizer_group(norm_t, 'norm_t',.) %>% 
  featurizer_group(norm_f, 'norm_f',.)
  


```

## Test - Train split

### Option 1: Blind partition:

```{r}
index = caret::createDataPartition(df$questionable_domain, p = 0.2, list = F)
train = df_ex[index,]
test = df_ex[-index,]
```

### Option 2: Restructure dataset:

```{r}
set.seed(123)
# true tweets:
index_t = which(df$questionable_domain)

# false tweets:
index_f = which(!df$questionable_domain) %>% 
  sample(6000)

index_r = c(index_t,index_t,index_f)

df_ex_r = df_ex[index_r,]
index = caret::createDataPartition(df_ex_r$questionable_domain, p = 0.2, list = F)

train = df_ex_r[-index,]
test = df_ex_r[index,]
```

## Models fit

```{r}
# common parameters:
y = test$questionable_domain

# modelo = as.formula(factor(y) ~ factor(user_verified) + factor(contains_profanity))

modelo = as.formula(factor(y) ~ factor(user_verified) + factor(contains_profanity) +
                      factor(`@gop`) + factor(biden) + factor(trump) + factor(elect) +
                      factor(`@gatewaypundit`) +
                      hash_f + hash_t + mention_f + mention_t + norm_f + norm_t)

ctrl = trainControl(method = "cv",
                      number = 10,
                      savePredictions = T)

# model-specific grid:
grelha = expand.grid(mtry = c(1,2,5,10,20,30,40,50))


```

## Naive Bayes

```{r}
model_nb = train(modelo,
                 data = train,
                 method = "naive_bayes",
                 trControl = ctrl)

model_nb
confusionMatrix(model_nb, mode = 'everything')
```

## Random Forest

```{r}
# corremos a simulação
model_rf = caret::train(modelo,
                     method="rf",
                     data=train,
                     trControl=ctrl,
                     #tuneGrid=grelha,
                     #preProc=c("BoxCox"),
                     ntree=100)



print(model_rf)
model_rf$finalModel
model_rf$results

```

## SVM

```{r}
model_svm =
train(modelo,
      data = train,
      method = "svmLinear",
      control = ctrl)

model_svm$finalModel
model_svm$pred
print(model_svm)


confusionMatrix(model_svm)
confusionMatrix(predict(model_svm, newdata = test),
                factor(test$questionable_domain),
                mode = 'everything')

```

## LDA

```{r}
model_lda = train(modelo,
                  data=train,
                  method="lda",
                  trControl = ctrl)

model_lda
```

## QDA - rank deficiency

```{r}
model_qda = train(modelo,
                  data=train,
                  method="qda",
                  trControl = ctrl)

model_qda
```


## Evaluation

```{r}
# https://intobioinformatics.wordpress.com/2019/11/26/how-to-easily-make-a-roc-curve-in-r/

#notas = evalm(list(model_rf, model_svm), gnames=c('rf','svm'))

```


# PREDICT NEW TWEETS

df_ex = featurizer('@gop', df) %>% 
  featurizer('biden',.) %>% 
    featurizer('trump',.) %>%
  featurizer('elect',.) %>% 
  featurizer('@gatewaypundit',.) %>% 
  featurizer_group(hash_f, 'hash_f',.) %>% 
  featurizer_group(hash_t, 'hash_t',.) %>% 
  featurizer_group(mention_t, 'mention_t',.) %>% 
  featurizer_group(mention_f, 'mention_f',.) %>% 
  featurizer_group(norm_t, 'norm_t',.) %>% 
  featurizer_group(norm_f, 'norm_f',.)
  


```{r, eval = F}

# pre-processing function
conversor = function(new, ml_model){
  temp = quanteda::corpus(new$description)
  tokens = quanteda::tokens(temp,
                        remove_punct = TRUE,
                        split_hyphens = TRUE)
  # builds output
  predictable = new %>% 
    mutate(`@gop` = ifelse('@gop' %in% tokens, 1,0),
           biden = ifelse('biden' %in% tokens, 1,0),
           elect = ifelse('elect' %in% tokens, 1,0),
           `@gatewaypundit`= ifelse('@gatewaypundit' %in% tokens, 1,0),
           hash_f = sum())
  
  
  if(ml_model == 'rf'){
      output = predict(model_rf, newdata = predictable)
  }
  else if(ml_model == 'svm'){
    output = predict(model_svm, newdata = predictable)
  }
  
  return(output)
}

# create dummy tweets
dummy = df[3,]

conversor(dummy, 'rf')

```






